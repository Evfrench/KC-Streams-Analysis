---
title: "KC Analysis Methods Draft"
author: "Em French"
date: "2023-05-04"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(forecast)
library(EGRET)
library(EGRETci)
library(MARSS)
library(marssTMB)
library(tidyverse)
library(dplyr)
library(data.table)
library(smwrBase)
library(timetk)
library(miscTools)
library(RSocrata)
source('./functions/get_socrata_data_func.R')
```

# Wrangle Data

```{r wrangle data}
####################################
#
# Start of Scripting and Plot Making 
#
####################################
  
# Query Socrata for the chosen site records, in this case Green River, Cedar River, and Issaquah Creek
# Need to... Call? Initialize? the funtions
GreenRiverData<- get_socrata_data_func(locns = c('A319'),
  parms = default_data_parms,
  SiteType = 'Streams and Rivers'
)

# We can normalize and make the reading more accessible py passing raw data through this function
GreenRiverDataExpanded = normalize_water_quality_data_parameters(GreenRiverData)

# Arrange the Rows by Collection date
GreenRiverDataExpanded <- arrange(GreenRiverDataExpanded,GreenRiverDataExpanded$CollectDate)

# There are two fields for DO, KC determine they have comparable detection results
# That means we can merge the two columns
GreenRiverDataExpanded$Dissolved_Oxygen <- GreenRiverDataExpanded$Dissolved_Oxygen + GreenRiverDataExpanded$`Dissolved_Oxygen,_Field`

# Merge the log values as well
GreenRiverDataExpanded$Dissolved_Oxygen_log <- GreenRiverDataExpanded$Dissolved_Oxygen_log + GreenRiverDataExpanded$`Dissolved_Oxygen,_Field_log`

# Remove one instance of duplicate values, will use the Dissolved_Oxygen sample because it was collected regularly at this point
GreenRiverDataExpanded$Dissolved_Oxygen[267] <- GreenRiverDataExpanded$Dissolved_Oxygen[267] - GreenRiverDataExpanded$`Dissolved_Oxygen,_Field`[267]
GreenRiverDataExpanded$Dissolved_Oxygen_log[267] <- GreenRiverDataExpanded$Dissolved_Oxygen_log[267] - GreenRiverDataExpanded$`Dissolved_Oxygen,_Field_log`[267]

# Load discharge records from cached CSV
green_river_daily_averages <- fread('./data_cache/green_river_daily_averages.csv')

```

# Initial Plots

These plot show a large gap from \~2008 to \~2012 There are a lot of 0's
in the data set because two columns are merged, these need to be removed

```{r River Plots}
#Create Dissolved Oxygen Plots

GreenRiverDataExpanded %>%
  ggplot(aes(x=CollectDate, y=Dissolved_Oxygen)) +
  geom_line() +
  ggtitle("Green River DO")

green_river_daily_averages %>%
  ggplot(aes(x=Date, y=LogQ)) +
  geom_line() +
  ggtitle("Green River Daily Flows")

```

## Formatting to fit the models

From the WRTDS_Sample_Workflow file, it looks like the sample dataframe
needs a number of fields that were not imported from the KC database, so
they have to be added manually

The initial window

```{r format data for WRTDS}
# Pull out Just the stuff we need and replace the 0's with empty values
GreenDO <- subset(GreenRiverDataExpanded, select = c(CollectDate, Year, Month, Dissolved_Oxygen))
GreenDO <- replace(GreenDO, GreenDO==0, NA)
GreenDO <- GreenDO %>% 
  rename(Date = CollectDate,
         ConcAve = Dissolved_Oxygen)

# Add all of the missing fields for WRTDS
GreenDO$ConcLow <- GreenDO$ConcAve
GreenDO$ConcHigh <- GreenDO$ConcAve
GreenDO$DecYear <- decimal_date(GreenDO$Date)
GreenDO$SinDY <- sin(2*pi*GreenDO$DecYear)
GreenDO$CosDY <- cos(2*pi*GreenDO$DecYear)
GreenDO$Uncen <- NA
GreenDO$Uncen <- c(1)
GreenDO$Date <- as_date(GreenDO$Date)
GreenDO$RefDate <- as_date("1850-01-01")
GreenDO$Julian<- difftime(GreenDO$Date, GreenDO$RefDate, units = "days")
GreenDO$Julian <- as.integer(GreenDO$Julian)
GreenDO$MonthSeq <- interval(GreenDO$RefDate, GreenDO$Date) %/% months(1)
GreenDO$waterYear <- waterYear(GreenDO$Date, numeric = FALSE)

#Quick plot of new data
GreenDO %>%
  ggplot(aes(x=Date, y=ConcAve)) +
  geom_line() +
  ggtitle("Green River DO")
```

Much better, This almost looks like the DO is steadily decreasing or
staying constant

# WRTDS

The model appears to converge. I still need to do some diagnostics

```{r WRTDS Attempt}

yr_frst <- as_date("1980-01-01")
yr_last <- as_date("2022-12-31")
GreenDO <- GreenDO[GreenDO[, "Date"] >= yr_frst & GreenDO[, "Date"] <= yr_last,]
row.names(GreenDO) <- NULL
# Gather discharge data:
siteID <- "12113000" #Green River in Auburn, WA

# Gather sample data:
parameter_cd<-"00300" #5 digit USGS code, this is the dissolved oxygen
#Sample <- readNWISSample(siteID,parameter_cd,startDate,endDate) We collected this from the 
#Sample <- generate_egret_sample_from_water_quality_data(GreenRiverDataExpanded, unique(GreenRiverData$Parameter)) Commenting out until it works
Sample <- na.omit(GreenDO) #The sample data cannot be retrieve from the NWIS because it 
Sample <- distinct(Sample)
# comes from King County
# Example of generating egret sample data from the expanded ds
#Sample = NULL
#Sample  = generate_egret_sample_from_water_quality_data(GreenRiverDataExpanded, "Total_Alkalinity")
row.names(Sample) <- NULL
# Gather discharge data:
# The Green River Gauge is actually 3.5km downstream from the King County site,
# Another creek merges with it during that time, so we must subtract those discharges
# The corrected discharge data were collected and stored in the cache, so we need to
# import the data and replace all the discharge values with those in the .csv

Daily <- readNWISDaily(siteID,"00060",yr_frst,yr_last)

Daily$Q <- green_river_daily_averages$Q
Daily$LogQ <- green_river_daily_averages$LogQ
Daily$Q7 <- green_river_daily_averages$Q7
Daily$Q30 <- green_river_daily_averages$Q30



# Gather site and parameter information:
# Here user must input some values for
# the default (interactive=TRUE)
INFO<- readNWISInfo(siteID,parameter_cd)
INFO$shortName <- "Green River at Aurburn, WA"

# Merge discharge with sample data:
eList <- mergeReport(INFO, Daily, Sample)

# Run WRTDS model:
eList <- modelEstimation(eList)

yeartest <- setupYears(eList$Daily)
# Test some of the plotting functions:
plotConcTimeDaily(eList)
genericEGRETDotPlot(x = yeartest$DecYear,
                    y = c(yeartest$Conc),
                    xlim = c(1995, 2022),
                    ylim = c(10, 12),
                    xlab = "Date",
                    ylab = "DO concentration")
genericEGRETDotPlot(x = yeartest$DecYear,
                    y = c(yeartest$FNConc),
                    xlim = c(1995, 2022),
                    ylim = c(10, 12),
                    xlab = "Date",
                    ylab = "Flow-normalized DO")
plotConcHist(eList, "1995","2015")

# Why aren't some of these working?
egretsurfaces <- eList[["surfaces"]]
EGRET::multiPlotDataOverview(eList = eList)
EGRET::dataOverview(Daily, Sample)
EGRET::plot15(eList, yr_frst, yr_last) #This function is not working currently
EGRET::errorStats(eList) 
```

The model has converged, excellent! Next steps: Dig into the "surfaces"
array and extract any data available. Targets: response variable
estimates, coefficient values, R-squared or other model fit data,
confidence intervals, residuals.

Stats known: R-squared = 0.792 for logDO concentration, R-squared =
0.997 for LogDO flux (how does this get calculated?)

# DLM

Everything should be in the correct form? The model took about half an
hour to run 1000 iterations but it failed to converge.

1st Attempt to fix: switch to marssTMB with the same data structure,
using TMB prevents me from setting the intercept to be constant Results:
MARSS still ran for 20 plus minutes so I quit before it finished

2nd Attempt: Use marssTMB and compress covariates down to monthly
averages Results: The model has converged! This does use only monthly
averages. I am a little concerned this model may be too "flattened" but
we will see

3rd Attempt: Changed window to 1975-2022, this is essentially the entire
record of Green River. The model output gave multiple NaN warnings and
failed to converge. (specifically, it gave a false convergence warning).

4th Attempt: Changed window to 1985-2022 in order to cut off some empty
fields. I need to get records of exactly when the tech boom and urban
consolidation. Best guess is 1990s? 5 years prior should work

```{r DLM Attempt}
# Merge the water quality (Sample) and the discharge (Daily) frames
# Discharge dataframe has daily records, while the quality frame has bi-monthly
# This means the response variable will have lots of missing entries
marsFrame <- data_frame()
marsFrame <- full_join(Sample, Daily, by = "Date") 
marsFrame <- arrange(marsFrame,marsFrame$Date)
marsFrame$SinDY <- sin(2*pi*marsFrame$DecYear.y) # Fill in the sin and cos of 
marsFrame$CosDY <- cos(2*pi*marsFrame$DecYear.y) # the decimal year to have daily reports

# Compress all the data into monthly averages
yearFrame <- marsFrame %>% mutate(month = as.Date(Date, "%B")) %>%
    group_by(MonthSeq.y) %>%
    summarize(decyear = as.double((mean(DecYear.y))))
QFrame <- marsFrame %>% mutate(month = as.Date(Date, "%B")) %>%
    group_by(MonthSeq.y) %>%
    summarize(LogQ = as.double(mean(LogQ))) 
sineFrame <- marsFrame %>%  mutate(month = as.Date(Date, "%B")) %>%
    group_by(MonthSeq.y) %>%
    summarize(SinDY = as.double(mean(SinDY)))
cosFrame <- marsFrame %>%  mutate(month = as.Date(Date, "%B")) %>%
    group_by(MonthSeq.y) %>%
    summarize(CosDY = as.double(mean(CosDY)))
DOFrame <- marsFrame %>%  mutate(month = as.Date(Date, "%B")) %>%
  group_by(MonthSeq.y) %>%
    summarize(logDO = as.double(log(mean(ConcAve, na.rm = TRUE))))

TT = nrow(DOFrame)

# Response variable: log(Dissolved Oxygen)
DOFrame$logDO[is.nan(DOFrame$logDO)] <- NA
DO = t(DOFrame$logDO)

#Covariates: Time(as the decimal year), Daily ave discharge (logQ), seasonal effects (sin and cos of the decimal year)
Z = array(NA,c(1,5,TT))
Z[1,1,] = c(1)
Z[1,2,] = yearFrame$decyear
Z[1,3,] = QFrame$LogQ
Z[1,4,] = sineFrame$SinDY
Z[1,5,] = cosFrame$CosDY

modlist1 = list( #all other arguments are left at default
  Z = Z,
  U = "zero",
  A = "zero",
  B = "identity",
  Q = ldiag(list(5e-6,"q2","q3","q4","q5"))
)

modlist2 = list( #all other arguments are left at default
  Z = Z,
  U = "zero",
  A = "zero",
  B = "identity",
  Q = "diagonal and unequal"
)
# initial effects were set to 0, maybe 1 would be better
# EEH: This approach is not working; need to use a year effect; see below
marssfit1 <- MARSS(DO, model = modlist1, inits = list(x0 = matrix(c(0),5,1)),  control = list(maxit=3000)
                , method = "TMB"
                 )

marssfit2 <- MARSS(DO, model = modlist2, inits = list(x0 = matrix(c(0),5,1)),  control = list(maxit=3000)
                , method = "TMB"
                 )
```

The Model has successfully converged, I would like to get a visual
comparison, look at r-squared values, and compare the change in effects
over time. (1976-2009)

Problem: With the fixed mean (level) in modlist1, there is no way to
track the changing mean yearly level. When I tried to force the mean to
vary.

```{r Residuals}
plotResidPred(eList)
#resids <- MARSSresiduals(marsfit)
plotConcTimeDaily(eList)
autoplot.marssMLE(marssfit1,silent = TRUE)

Z2=Z
dim(Z2)= c(5,dim(Z)[3])
A = Z2[,1:3] %*% marssfit1$states[1:3,]
plot(diag(A))

TT=dim(Z)[3]
lev = rep(NA, TT)
for (i in 1:TT){
  lev[i] = Z[1, 1:3,i] %*% marssfit1$states[1:3,i]
}
plot(lev)
apply(Z,2,function(x){any(is.na(x))})

DOFrame$States <- t(marssfit1$states)
DOFrame$Est <- DOFrame$States[,"X1"]*Z[1,1,] + DOFrame$States[,"X2"]*yearFrame$decyear + DOFrame$States[,"X3"]*QFrame$LogQ + 
DOFrame$States[,"X4"]*sineFrame$SinDY + DOFrame$States[,"X5"]*cosFrame$CosDY
DOFrame2 <- na.omit(DOFrame)
rSquared(DOFrame2$logDO, DOFrame2$logDO - DOFrame2$Est) #use this to calculate R-squared value for the MARSS Model(I hope)
```
Note: R-squared for DLM is 0.7988926, this is better than the EGRET model


## Allow the level to vary

The problem with `modlist2` is that the mean level of the data is not
being tracked. The best model (per AIC) is a flat level. But we can
clearly see that the mean level is changing. So we can force a
time-varying level but we need to use a small enough Q so that it
doesn't track the seasonal cycles.

A Q of `1e-6` seems roughly right.

```         
modlist1 = list( #all other arguments are left at default
  Z = Z,
  U = "zero",
  A = "zero",
  B = "identity",
  Q = ldiag(list(1e-6,"q2","q3","q4","q5"))
)
```

Now that we are tracking the mean level, we see something interesting.
The timing of the peak is shifting (X4) and something odd happened in
the amplitude of the seasonal cycle around time 200. I think that you
can remove the linear year effect (X2). That will be captured by the
time-varying level (X1). Regarding X3, the inference is that yes, this
variable has an effect but that effect is not changing over time.

```{r}
autoplot.marssMLE(marssfit1, plot.type="xtT")
autoplot.marssMLE(marssfit2, plot.type="xtT")
```

## Compute how the peak is moving

Peaks are when the derivative is equal to zero. So we want to solve for
the $t$ where

$$(a \times sin(2 \pi t/12) + b \times cos(2 \pi t/12) )^\prime = 0$$
$$ a \times cos(2 \pi t /12) - b \times sin(2 \pi t/12) = 0$$

$$tan(2 \pi t/12) = a/b$$

$$t = \frac{12 \times atan(a/b)}{2 \pi}$$ Solving for this is a bit odd
because as the angle changes through $\pi/2$ the `atan()` keeps jumping
around. This function gets the location of the peak given $a$ and $b$
where seasonal model is $a sin() + b cos()$.

```{r}
# x is a vector of 2 numbers
findpeak <- function(x){
  a <- x[1]; b <- x[2]
  if(a > 0 & b > 0) angle <- atan(a/b)
  if(a > 0 & b < 0) angle <- pi/2 + atan(a/-b)
  if(a < 0 & b < 0) angle <- pi + atan(-a/-b)
  if(a < 0 & b > 0) angle <- 3*pi/2 + atan(-a/b)
  if(a == 0 & b > 0) angle <- 0
  if(a == 0 & b < 0) angle <- pi
  if(b == 0 & a > 0) angle <- pi/2
  if(b == 0 & a < 0) angle <- 3*pi/2
  if(a == 0 & b == 0) angle <- NA
  return(12*angle/(2*pi))
}
```

So it looks like something happened and the peak shifted by a few days.
Scale is on y axis is months.

```{r}
peak <- apply(marssfit1$states[4:5,],2,findpeak)
plot(peak, type="l")
```

You can also look now at the amplitude: low to high. This one is easy.
Amplitude is $a^2 + b^2$. This one shows that the cycle amplitude has
generally stayed the same except whatever weird thing happened.

```{r}
amplitude <- sqrt(marssfit1$states[4,]^2 + marssfit1$states[5,]^2)
plot(amplitude, type="l")
```

## MARSS DLM Residuals:

(1976-2009) Response residuals appear to have tails indicating the error
distribution is not Gaussian. States (the effects matrix for covariates)
appear to be mostly constant except for x3 (log-discharge) and x5
(cosine seasonality). x5 residuals have a significant tail at the lower
end. This appears to be caused by the large dip around time point 350

(1985-2022) Both ends of the response residuals are low Further plan:
This analysis will focus on temperature and DO of KC water bodies. Our
hypothesis is that water quality metrics have been staying constant
despite increasing temperature, and this is due to improvements in water
management practices such as increased stormwater infrastrucure, more
awareness of water quality by locals, etc. (quantifying this will be
challenging, as of now we need to determine if there even is a long term
trend)

Addendum: Move the window of years you want to analyze to more recent
times. Apparently the missing 4 years should not bias the data too much,
at least for the MARSS model. Need to check how this missing data will
affect the WRTDS model.
